<!doctype html><html><head><link rel=stylesheet href=https://hybrid-cloud-patterns.io/sass/patternfly.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://hybrid-cloud-patterns.io/sass/patternfly-addons.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/solid.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/fontawesome.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/brands.css><title>Hybrid Cloud Patterns | Hybrid Cloud Patterns</title></head><body><div class=pf-c-page><header class=pf-c-page__header><div class=pf-c-page__header-brand><a href=/ class=pf-c-page__header-brand-link><img src=/images/hybrid_cloud_patterns.png alt="Hybrid Cloud Patterns"></a></div></header><div class=pf-c-page__sidebar><div class=pf-c-page__sidebar-body><div class="pf-c-nav pf-m-light"><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/about/ class="pf-c-nav__link pf-m-current">Hybrid Cloud Patterns</a><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/quickstart/ class=pf-c-nav__link>Patterns quick start
<span class=pf-c-accordion__toggle-icon><i class="fas fa-angle-right" aria-hidden=true></i></span></a><section class="pf-c-nav__subnav collapse"><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/importing-a-cluster/ class=pf-c-nav__link>Importing a cluster</a></li></ul></section></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/infrastructure/ class=pf-c-nav__link>Infrastructure
<span class=pf-c-accordion__toggle-icon><i class="fas fa-angle-right" aria-hidden=true></i></span></a><section class="pf-c-nav__subnav collapse"><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/ocp-cluster-general-sizing/ class=pf-c-nav__link>OpenShift General Sizing</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/rhel-for-edge-general-sizing/ class=pf-c-nav__link>RHEL for Edge General Sizing</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/using-validated-pattern-operator/ class=pf-c-nav__link>Using the Validated Pattern operator</a></li></ul></section></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/workflow/ class=pf-c-nav__link>Workflow
<span class=pf-c-accordion__toggle-icon><i class="fas fa-angle-right" aria-hidden=true></i></span></a><section class="pf-c-nav__subnav collapse"><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/implementation/ class=pf-c-nav__link>Implementation Requirements</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/community/ class=pf-c-nav__link>Community Patterns</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/validated/ class=pf-c-nav__link>Validated Patterns</a></li></ul></section></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/secrets/ class=pf-c-nav__link>Secrets
<span class=pf-c-accordion__toggle-icon><i class="fas fa-angle-right" aria-hidden=true></i></span></a><section class="pf-c-nav__subnav collapse"><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/vault/ class=pf-c-nav__link>HashiCorp Vault</a></li></ul></section></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/faq/ class=pf-c-nav__link>FAQ</a></ul></div></div></div><main class=pf-c-page__main tabindex=0 data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=50><section class="pf-c-page__main-section pf-m-fill"><div class=pf-u-display-flex><div class=pf-c-content><h1 id=hybrid-cloud-patterns>Hybrid Cloud Patterns</h1><p>Hybrid Cloud Patterns and the downstream Validated Patterns are a natural progression from reference architectures with additional value. Here is a brief video to explain what patterns are all about:</p><p><a href="https://www.youtube.com/watch?v=lI8TurakeG4"><img src=https://img.youtube.com/vi/lI8TurakeG4/0.jpg alt=patterns-intro-video></a></p><p>This effort is focused on customer solutions that involve multiple Red Hat
products. The patterns include one or more applications that are based on successfully deployed customer examples. Example application code is provided as a demonstration, along with the various open source projects and Red Hat products required to for the deployment to work. Users can then modify the pattern for their own specific application.</p><p>How do we select and produce a pattern? We look for novel customer use cases, obtain an open source demonstration of the use case, validate the pattern with its components with the relevant product engineering teams, and create GitOps based automation to make them easily repeatable and extendable.</p><p>The automation also enables the solution to be added to Continuous Integration (CI), with triggers for new product versions (including betas), so that we can proactively find and fix breakage and avoid bit-rot.</p><h2 id=who-should-use-these-patterns>Who should use these patterns?</h2><p>It is recommended that architects or advanced developers with knowledge of Kubernetes and Red Hat OpenShift Container Platform use these patterns. There are advanced <a href=https://www.cncf.io/projects/>Cloud Native</a> concepts and projects deployed as part of the pattern framework. These include, but are not limited to, OpenShift Gitops (<a href=https://argoproj.github.io/argo-cd/>ArgoCD</a>), Advanced Cluster Management (<a href=https://open-cluster-management.io/>Open Cluster Management</a>), and OpenShift Pipelines (<a href=https://tekton.dev/>Tekton</a>)</p><h2 id=general-structure>General Structure</h2><p>All patterns assume an OpenShift cluster is available to deploy the application(s) that are part of the pattern. If you do not have an OpenShift cluster, you can use <a href=https://console.redhat.com/openshift>cloud.redhat.com</a>.</p><p>The documentation will use the <code>oc</code> command syntax but <code>kubectl</code> can be used interchangeably. For each deployment it is assumed that the user is logged into a cluster using the <code>oc login</code> command or by exporting the <code>KUBECONFIG</code> path.</p><p>The diagram below outlines the general deployment flow of a datacenter application.</p><p>But first the user must create a fork of the pattern repository. This allows changes to be made to operational elements (configurations etc.) and to application code that can then be successfully made to the forked repository for DevOps continuous integration (CI). Clone the directory to your laptop/desktop. Future changes can be pushed to your fork.</p><p><img src=/images/gitops-datacenter.png alt="GitOps for Datacenter"></p><ol><li><p>Make a copy of the values file. There may be one or more values files. E.g. <code>values-global.yaml</code> and/or <code>values-datacenter.yaml</code>. While most of these values allow you to specify subscriptions, operators, applications and other application specifics, there are also <em>secrets</em> which may include encrypted keys or user IDs and passwords. It is important that you make a copy and <strong>do not push your personal values file to a repository accessible to others!</strong></p></li><li><p>Deploy the application as specified by the pattern. This may include a Helm command (<code>helm install</code>) or a make command (<code>make deploy</code>).</p></li></ol><p>When the workload is deployed the pattern first deploys OpenShift GitOps. OpenShift GitOps will then take over and make sure that all application and the components of the pattern are deployed. This includes required operators and application code.</p><p>Most patterns will have an Advanced Cluster Management operator deployed so that multi-cluster deployments can be managed.</p><h2 id=edge-patterns>Edge Patterns</h2><p>Some patterns include both a data center and one or more edge clusters. The diagram below outlines the general deployment flow of applications on an edge application. The edge OpenShift cluster is often deployed on a smaller cluster than the datacenter. Sometimes this might be a three node cluster that allows workloads to be deployed on the master nodes. The edge cluster might be a single node cluster (SN0). It might be deployed on bare metal, on local virtual machines or in a public/private cloud. Provision the cluster (see above)</p><p><img src=/images/gitops-edge.png alt="GitOps for Edge"></p><ol start=3><li>Import/join the cluster to the hub/data center. Instructions for importing the cluster can be found [here]. You&rsquo;re done.</li></ol><p>When the cluster is imported, ACM on the datacenter will deploy an ACM agent and agent-addon pod into the edge cluster. Once installed and running ACM will then deploy OpenShift GitOps onto the cluster. Then OpenShift GitOps will deploy whatever applications are required for that cluster based on a label.</p><h2 id=openshift-gitops-aka-argocd>OpenShift GitOps (a.k.a ArgoCD)</h2><p>When OpenShift GitOps is deployed and running in a cluster (datacenter or edge) you can launch its console by choosing ArgoCD in the upper left part of the OpenShift Console (TO-DO whenry to add an image and clearer instructions here)</p></div><aside class="pf-c-jump-links pf-m-vertical sticky pf-m-expandable pf-m-non-expandable-on-2xl" aria-label="Table of contents"><div class=pf-c-jump-links__header><div class=pf-c-jump-links__label><h1>Table of Contents</h1></div></div><nav id=TableOfContents><ul class=pf-c-jump-links__list><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#hybrid-cloud-patterns>Hybrid Cloud Patterns</a><ul class=pf-c-jump-links__list><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#who-should-use-these-patterns>Who should use these patterns?</a></li><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#general-structure>General Structure</a></li><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#edge-patterns>Edge Patterns</a></li><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#openshift-gitops-aka-argocd>OpenShift GitOps (a.k.a ArgoCD)</a></li></ul></li></ul></nav></aside></div></section><section id=prefooter class="pf-c-page__main-section footer-dark"><div class="pf-l-grid prefooter-menu-grid"><div class="pf-l-grid__item pf-m-12-col-on-sm pf-m-6-col-on-md pf-m-offset-1-col-on-md pf-u-mb-lg pf-u-mb-0-on-sm"><div class="pf-l-grid pf-u-py-xl"><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-ml-md pf-u-ml-0-on-md pf-u-mb-xl pf-u-mb-0-on-md"><p class="pf-c-title footer-menu-title">QUICKLINKS</p><nav aria-label="Quick Links"><ul class="pf-c-list pf-m-plain"><li><i class="prefooter-icon fa-brands fa-github"></i>
<a class=footer-link aria-label="GitHub repository" href=https://github.com/hybrid-cloud-patterns>GitHub repository</a></li><li><i class="prefooter-icon fas fa-check-circle"></i>
<a class=footer-link aria-label="Available patterns" href=/patterns>Available patterns</a></li></ul></nav></div><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-mt-lg pf-u-mt-0-on-sm pf-u-ml-md pf-u-ml-0-on-md pf-u-mb-xl pf-u-mb-0-on-md"><p class="pf-c-title footer-menu-title">CONTRIBUTE</p><nav aria-label=Contribute><ul class="pf-c-list pf-m-plain ws-org-pfsite-footer-menu-list"><li><i class="prefooter-icon fas fa-pen"></i>
<a class=footer-link aria-label="Documentation contributor guidelines" href=/contribute/contribute-to-docs/>Documentation</a></li><li><i class="prefooter-icon fas fa-code"></i>
<a class=footer-link aria-label="How to create a new pattern" href=/contribute/creating-a-pattern/>Patterns</a></li></ul></nav></div><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-mt-lg pf-u-mt-0-on-md pf-u-ml-md pf-u-ml-0-on-md"><p class="pf-c-title footer-menu-title">STAY IN TOUCH</p><nav aria-label="Stay in touch"><ul class="pf-c-list pf-m-plain"><li><i class="prefooter-icon fas fa-envelope"></i>
<a href=https://groups.google.com/g/hybrid-cloud-patterns class=footer-link target=top aria-label="Join the Validated Patterns mailing list">Mailing list</a></li></ul></nav></div></div></div><div class="pf-l-grid__item pf-m-12-col-on-sm pf-m-4-col-on-md"><div class="pf-l-grid pf-u-pt-xl"><div class="pf-l-grid__item pf-u-px-xl"><a class="pf-c-page__header-brand-link pf-c-brand pf-u-pb-md" href=/><img class=pf-c-brand src=/images/hybrid_cloud_patterns.png alt="Hybrid Cloud Patterns"></a><p class=footer-link>Hybrid Cloud Patterns are an evolution of how you deploy applications in a hybrid cloud. With a pattern, you can automatically deploy a full application stack through a GitOps-based framework. With this framework, you can create business-centric solutions while maintaining a level of Continuous Integration (CI) over your application.</p></div></div></div></div></section><footer id=footer class="footer-dark pf-m-no-fill pf-l-flex footer-center"><div class=pf-l-flex__item><a href=//www.redhat.com target=top aria-label="Visit Red Hat.com"><img src=/images/RHlogo.svg alt="Red Hat logo" width=145px height=613px></a><span class=site-copyright>Copyright &copy; 2023 Red Hat, Inc.</span></div><script src=/js/bootstrap.bundle.js></script>
<script>window.store={"https://hybrid-cloud-patterns.io/learn/about/":{title:"Hybrid Cloud Patterns",tags:[],content:`Hybrid Cloud Patterns Hybrid Cloud Patterns and the downstream Validated Patterns are a natural progression from reference architectures with additional value. Here is a brief video to explain what patterns are all about:
This effort is focused on customer solutions that involve multiple Red Hat products. The patterns include one or more applications that are based on successfully deployed customer examples. Example application code is provided as a demonstration, along with the various open source projects and Red Hat products required to for the deployment to work. Users can then modify the pattern for their own specific application.
How do we select and produce a pattern? We look for novel customer use cases, obtain an open source demonstration of the use case, validate the pattern with its components with the relevant product engineering teams, and create GitOps based automation to make them easily repeatable and extendable.
The automation also enables the solution to be added to Continuous Integration (CI), with triggers for new product versions (including betas), so that we can proactively find and fix breakage and avoid bit-rot.
Who should use these patterns? It is recommended that architects or advanced developers with knowledge of Kubernetes and Red Hat OpenShift Container Platform use these patterns. There are advanced Cloud Native concepts and projects deployed as part of the pattern framework. These include, but are not limited to, OpenShift Gitops (ArgoCD), Advanced Cluster Management (Open Cluster Management), and OpenShift Pipelines (Tekton)
General Structure All patterns assume an OpenShift cluster is available to deploy the application(s) that are part of the pattern. If you do not have an OpenShift cluster, you can use cloud.redhat.com.
The documentation will use the oc command syntax but kubectl can be used interchangeably. For each deployment it is assumed that the user is logged into a cluster using the oc login command or by exporting the KUBECONFIG path.
The diagram below outlines the general deployment flow of a datacenter application.
But first the user must create a fork of the pattern repository. This allows changes to be made to operational elements (configurations etc.) and to application code that can then be successfully made to the forked repository for DevOps continuous integration (CI). Clone the directory to your laptop/desktop. Future changes can be pushed to your fork.
Make a copy of the values file. There may be one or more values files. E.g. values-global.yaml and/or values-datacenter.yaml. While most of these values allow you to specify subscriptions, operators, applications and other application specifics, there are also secrets which may include encrypted keys or user IDs and passwords. It is important that you make a copy and do not push your personal values file to a repository accessible to others!
Deploy the application as specified by the pattern. This may include a Helm command (helm install) or a make command (make deploy).
When the workload is deployed the pattern first deploys OpenShift GitOps. OpenShift GitOps will then take over and make sure that all application and the components of the pattern are deployed. This includes required operators and application code.
Most patterns will have an Advanced Cluster Management operator deployed so that multi-cluster deployments can be managed.
Edge Patterns Some patterns include both a data center and one or more edge clusters. The diagram below outlines the general deployment flow of applications on an edge application. The edge OpenShift cluster is often deployed on a smaller cluster than the datacenter. Sometimes this might be a three node cluster that allows workloads to be deployed on the master nodes. The edge cluster might be a single node cluster (SN0). It might be deployed on bare metal, on local virtual machines or in a public/private cloud. Provision the cluster (see above)
Import/join the cluster to the hub/data center. Instructions for importing the cluster can be found [here]. You&rsquo;re done. When the cluster is imported, ACM on the datacenter will deploy an ACM agent and agent-addon pod into the edge cluster. Once installed and running ACM will then deploy OpenShift GitOps onto the cluster. Then OpenShift GitOps will deploy whatever applications are required for that cluster based on a label.
OpenShift GitOps (a.k.a ArgoCD) When OpenShift GitOps is deployed and running in a cluster (datacenter or edge) you can launch its console by choosing ArgoCD in the upper left part of the OpenShift Console (TO-DO whenry to add an image and clearer instructions here)
`,url:"https://hybrid-cloud-patterns.io/learn/about/",breadcrumb:"/learn/about/"},"https://hybrid-cloud-patterns.io/learn/quickstart/":{title:"Patterns quick start",tags:[],content:` Patterns quick start Each pattern can be deployed using the command line. The only requirement is to have git and podman installed. See the Prerequisite installation instructions for more information.
Patterns deployment requires several tools including Helm to install. However, the validated patterns framework removes the need to install and maintain these tools. The pattern.sh script uses a container the includes the necessary tools. The use of that container is why you need to install podman.
Check the values-*.yaml for changes that are needed before deployment. After changing the values-*.yaml files where needed and pushing them to your git repository, you can run ./pattern.sh make install from your local repository directory and that will deploy the datacenter/hub cluster for a pattern. Edge clusters are deployed by joining/importing them into ACM on the hub.
Alternatively to the ./pattern.sh make install method, you can use the validated pattern operator available in the OpenShift console.
For information on using the Validated Patterns Operator, see Using the Validated Pattern Operator.
Follow any other post-install instructions for the pattern on that pattern’s Getting started page.
Prerequisite installation instructions Tested Operating systems The following instructions have been tested on the following operating systems:
Red Hat Enterprise Linux 8 and 9
CentOS 8 and 9
Fedora 36 and onwards
Debian Bookworm
Ubuntu 22.04
Mac OSX Big Sur and onwards
Red Hat Enterprise Linux 8 and 9 Make sure that you have both the appstream and the baseos repositories configured. For example on RHEL 8 you will get the following:
sudo dnf repolist Updating Subscription Management repositories. repo id repo name rhel-8-for-x86_64-appstream-rpms Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs) rhel-8-for-x86_64-baseos-rpms Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs) Install podman and git:
sudo dnf install -y podman git Fedora Install podman and git:
sudo dnf install -y podman git Debian and derivatives Install podman and git:
sudo apt-get install -y podman git Mac OSX Install podman and git:
/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; brew install podman git # Containers on MacOSX run in a VM which is managed by &#34;podman machine&#34; commands podman machine init -v \${HOME}:\${HOME} -v /private/tmp/:/private/tmp podman machine start `,url:"https://hybrid-cloud-patterns.io/learn/quickstart/",breadcrumb:"/learn/quickstart/"},"https://hybrid-cloud-patterns.io/learn/importing-a-cluster/":{title:"Importing a cluster",tags:[],content:` Importing a managed cluster Many validated patterns require importing a cluster into a managed group. These groups have specific application sets that will be deployed and managed. Some examples are factory clusters in the Industrial Edge pattern, or development clusters in Multi-cluster DevSecOps pattern.
Red Hat Advanced Cluster Management (RHACM) can be used to create a cluster of a specific cluster group type. You can deploy a specific cluster that way if you have RHACM set up with credentials for deploying clusters. However in many cases an OpenShift cluster has already been created and will be imported into the set of clusters that RHACM is managing.
While you can create and deploy in this manner this section concentrates on importing an existing cluster and designating a specific managed cluster group type.
To deploy a cluster that can be imported into RHACM, use the openshift-install program provided at console.redhat.com. You will need login credentials.
Importing a cluster using the RHACM User Interface Getting to the RHACM user interface After ACM is installed a message regarding a &#34;Web console update is available&#34; will be displayed. Click on the &#34;Refresh web console&#34; link.
On the upper-left side you’ll see a pull down labeled &#34;local-cluster&#34;. Select &#34;All Clusters&#34; from this pull down. This will navigate to the RHACM console and to its &#34;Clusters&#34; section
Select the &#34;Import cluster&#34; option.
Importing the cluster On the &#34;Import an existing cluster&#34; page, enter the cluster name (arbitrary) and choose Kubeconfig as the &#34;import mode&#34;. Add the tag clusterGroup= using the appropriate cluster group specified in the pattern. Press Import.
Using this method, you are done. Skip to the section in your pattern documentation that describes how you can confirm the pattern deployed correctly on the managed cluster.
Other potential import tools There are a two other known ways to join a cluster to the RHACM hub. These methods are not supported but have been tested once. The patterns team no longer tests these methods. If these methods become supported we will maintain the documentation here.
Using the cm-cli tool
Using the clusteradm tool
Importing a cluster using the cm-cli tool Install the cm-cli (cm) (cluster management) command-line tool. See installation instructions here: cm-cli installation
Obtain the KUBECONFIG file from the managed cluster.
On the command-line login into the hub/datacenter cluster (use oc login or export the KUBECONFIG).
Run the following command:
cm attach cluster --cluster &lt;cluster-name&gt; --cluster-kubeconfig &lt;path-to-KUBECONFIG&gt; Importing a cluster using clusteradm tool You can also use clusteradm to join a cluster. The following instructions explain what needs to be done. clusteradm is still in testing.
To deploy a edge cluster you will need to get the hub/datacenter cluster’s token. You will need to install clusteradm. When it is installed run the following on existing hub/datacenter cluster:
clusteradm get token When you run the clusteradm command above it replies with the token and also shows you the command to use on the managed. Login to the managed cluster with either of the following methods:
oc login or
export KUBECONFIG=~/&lt;path-to-kubeconfig&gt; Then request to that the managed join the datacenter hub.
clusteradm join --hub-token &lt;token from clusteradm get token command &gt; &lt;managed-cluster-name&gt; Back on the hub cluster accept the join request.
clusteradm accept --clusters &lt;managed-cluster-name&gt; Designate the new cluster as a devel site If you use the command line tools above you need to explicitly indicate that the imported cluster is part of a specific clusterGroup. If you haven’t tagged the cluster as clusterGroup=&lt;managed-cluster-group&gt; then do that now. Some examples of clusterGroup are factory, devel, or prod.
We do this by adding the label referenced in the managedSite’s clusterSelector.
Find the new cluster.
oc get managedclusters.cluster.open-cluster-management.io Apply the label.
oc label managedclusters.cluster.open-cluster-management.io/&lt;your-cluster&gt; clusterGroup=&lt;managed-cluster-group&gt; `,url:"https://hybrid-cloud-patterns.io/learn/importing-a-cluster/",breadcrumb:"/learn/importing-a-cluster/"},"https://hybrid-cloud-patterns.io/learn/infrastructure/":{title:"Infrastructure",tags:[],content:`Infrastructure Background Each validated pattern has infrastructure requirements. The majority of the validated patterns will run Red Hat OpenShift while some parts will run directly on Red Hat Enterprise Linux or (RHEL), more likely, a version of RHEL called RHEL for Edge. It is expected that consumers of validated patterns already have the infrastructure in place using existing reliable and supported deployment tools. For more information and tools head over to console.redhat.com
Sizing In this section we provide general minimum sizing requirements for such infrastructure but it is important to review specific requirements for a specific validated pattern. For example, Industrial Edge 2.0 employs AI/Ml technology that requires large machine instances to support the applications deployed on OpenShift at the datacenter.
`,url:"https://hybrid-cloud-patterns.io/learn/infrastructure/",breadcrumb:"/learn/infrastructure/"},"https://hybrid-cloud-patterns.io/learn/ocp-cluster-general-sizing/":{title:"OpenShift General Sizing",tags:[],content:`OpenShift General Sizing Recommended node host practices The OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: podsPerCore and maxPods.
When both options are in use, the lower of the two values limits the number of pods on a node. Exceeding these values can result in:
Increased CPU utilization. Slow pod scheduling. Potential out-of-memory scenarios, depending on the amount of memory in the node. Exhausting the pool of IP addresses. Resource overcommitting, leading to poor user application performance. In Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.
podsPerCore sets the number of pods the node can run based on the number of processor cores on the node. For example, if podsPerCore is set to 10 on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.
kubeletConfig: podsPerCore: 10 Setting podsPerCore to 0 disables this limit. The default is 0. podsPerCore cannot exceed maxPods.
maxPods sets the number of pods the node can run to a fixed value, regardless of the properties of the node.
kubeletConfig: maxPods: 250 For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
Control plane node sizing The control plane node resource requirements depend on the number of nodes in the cluster. The following control plane node size recommendations are based on the results of control plane density focused testing. The control plane tests create the following objects across the cluster in each of the namespaces depending on the node counts:
12 image streams 3 build configurations 6 builds 1 deployment with 2 pod replicas mounting two secrets each 2 deployments with 1 pod replica mounting two secrets 3 services pointing to the previous deployments 3 routes pointing to the previous deployments 10 secrets, 2 of which are mounted by the previous deployments 10 config maps, 2 of which are mounted by the previous deployments Number of worker nodes Cluster load (namespaces) CPU cores Memory (GB) 25 500 4 16 100 1000 8 32 250 4000 16 96 On a cluster with three masters or control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted or fails because the remaining two nodes must handle the load in order to be highly available. This is also expected during upgrades because the masters are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures on large and dense clusters, keep the overall resource usage on the master nodes to at most half of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the master nodes accordingly.
The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the running phase.
If you used an installer-provisioned infrastructure installation method, you cannot modify the control plane node size in a running OpenShift Container Platform 4.5 cluster. Instead, you must estimate your total node count and use the suggested control plane node size during installation.
The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShiftSDN as the network plug-in.
In OpenShift Container Platform 4.5, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.
For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
Recommended etcd practices For large and dense clusters, etcd can suffer from poor performance if the keyspace grows excessively large and exceeds the space quota. Periodic maintenance of etcd, including defragmentation, must be performed to free up space in the data store. It is highly recommended that you monitor Prometheus for etcd metrics and defragment it when required before etcd raises a cluster-wide alarm that puts the cluster into a maintenance mode, which only accepts key reads and deletes. Some of the key metrics to monitor are etcd_server_quota_backend_bytes which is the current quota limit, etcd_mvcc_db_total_size_in_use_in_bytes which indicates the actual database usage after a history compaction, and etcd_debugging_mvcc_db_total_size_in_bytes which shows the database size including free space waiting for defragmentation. Instructions on defragging etcd can be found in the Defragmenting etcd data section.
Etcd writes data to disk, so its performance strongly depends on disk performance. Etcd persists proposals on disk. Slow disks and disk activity from other processes might cause long fsync latencies, causing etcd to miss heartbeats, inability to commit new proposals to the disk on time, which can cause request timeouts and temporary leader loss. It is highly recommended to run etcd on machines backed by SSD/NVMe disks with low latency and high throughput.
Some of the key metrics to monitor on a deployed OpenShift Container Platform cluster are p99 of etcd disk write ahead log duration and the number of etcd leader changes. Use Prometheus to track these metrics. etcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms.
For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
`,url:"https://hybrid-cloud-patterns.io/learn/ocp-cluster-general-sizing/",breadcrumb:"/learn/ocp-cluster-general-sizing/"},"https://hybrid-cloud-patterns.io/learn/rhel-for-edge-general-sizing/":{title:"RHEL for Edge General Sizing",tags:[],content:`RHEL for Edge General Sizing Recommended node host practices TBD
`,url:"https://hybrid-cloud-patterns.io/learn/rhel-for-edge-general-sizing/",breadcrumb:"/learn/rhel-for-edge-general-sizing/"},"https://hybrid-cloud-patterns.io/learn/using-validated-pattern-operator/":{title:"Using the Validated Pattern operator",tags:[],content:`Using the Validated Pattern Operator Background The Validated Pattern Operator was developed in order to take advantage of the automation provided with the validated pattern framework. It allows users to deploy validated patterns using the Red Hat OpenShift console. The idea is to point it at a validated pattern repo and let the operator do the work of deploying OpenShift Gitops and then all the parts and applications required for the pattern.
Installing the operator In the OpenShift console select Operators on the menu on the left of the console. Then select OperatorHub.
In the Filter by keyword box type the word validated. The validated patterns operator will appear. Select it. At this time it is a community operator and is not officially supported. A Community Operator information pop-up appears. Click Continue.
On the following pop-up page select the Install button.
On the Install Operator page leave the defaults and select install.
)
Create a pattern instance When the operator has completed installation click on View Operator. Then select the Create instance link and start filling out the Create a Pattern form.
Choose a name for the pattern deployment. This name will be used in the projects created. Apply any other labels you may need to this deployment. Choose a cluster group name. This is important because it identifies the type of cluster that this pattern will be deployed on. For example if this is Industrial Edge then it should be datacenter. If it&rsquo;s multicloud-gitops then it should be hub. Please check with the validated pattern to figure out which cluster group is right for this pattern. Select the Git Spec drop down on the form and change the Target Repo URL to your forked repository URL. E.g. From https://github.com/hybrid-cloud-patterns/pattern-name to https://github.com/my-git-user/pattern-name You may need to change the Target Revision sometimes it&rsquo;s marked stable or has a specific a version number e.g. v2.1, it may simply be main, or it might be a new branch you&rsquo;ve created, my-branch. Make sure to make any necessary changes to your values-*.yaml files locally and push them to your forked repo. on the correct branch/target chosen above. For a quick start you shouldn&rsquo;t need to make changes. Review the rest of the form fields and check if they require changes. For first time pattern deployments you probably don&rsquo;t need any further changes. Select Create and the bottom of the form. The OpenShift GitOps operator should show up in Installed Operators momentarily.
From there OpenShift GitOps will install the rest of the assets and artifacts for this pattern. Make sure to change your project to All Projects so you will see the other operators installing. E.g. Advanced Cluster Management (ACM).
Please follow any other post-install instructions for the pattern on that pattern&rsquo;s Getting started page.
`,url:"https://hybrid-cloud-patterns.io/learn/using-validated-pattern-operator/",breadcrumb:"/learn/using-validated-pattern-operator/"},"https://hybrid-cloud-patterns.io/learn/workflow/":{title:"Workflow",tags:[],content:`Workflow These patterns are designed to be composed of multiple components, and for those components to be used in gitops workflows by consumers and contributors. To use the first pattern as an example, we maintain the Industrial Edge pattern, which uses a repo with pattern-specific logic and configuration as well as a common repo which has elements common to multiple patterns. The common repository is included in each pattern repository as a subtree.
Consuming a pattern Fork the pattern repository on GitHub to your workspace (GitHub user or organization). It is necessary to fork because your fork will be updated as part of the GitOps and DevOps processes, and the main branch (by default) will be used in the automated workflows.
Clone the forked copy
git clone git@github.com:&lt;your-workspace&gt;/industrial-edge.git
Create a local copy of the Helm values file that can safely include credentials
DO NOT COMMIT THIS FILE You do not want to push personal credentials to GitHub.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml Customize the deployment for your cluster
vi values-global.yaml git commit values-global.yaml git push Contributing For contributions, we recommend adding the upstream repository as an additional remote, and making changes on a branch other than main. Changes on this branch can then be merged to the main branch (to be reflected in the GitOps workflows) and will be easier to make upstream, if you wish. Contributions from your forked main branch will contain, by design:
Customizations to values-global.yaml and other files that are particular to your installation Commits made by Tekton and other automated processes that will be particular to your installation To isolate changes for upstreaming (hcp is &ldquo;Hybrid Cloud Patterns&rdquo;, you can use a different remote and/or branch name if you want):
git remote add hcp https://github.com/hybrid-cloud-patterns/industrial-edge git fetch --all git branch -b hcp-main -t hcp/main &lt;make changes on the hcp-main branch&gt; git push origin hcp-main To update branch hcp-main with upstream changes:
git checkout hcp-main git pull --rebase To reflect these changes in your forked repository (such as if you would like to submit a PR later):
git push origin hcp-main If you want to integrate upstream pattern changes into your local GitOps process:
git checkout main git merge hcp-main git push origin main Using this workflow, the hcp-main branch will:
Be isolated from any changes that are being made by your local GitOps processes Be merge-able (or cherry-pick-able) into your local main branch to be used by your local GitOps processes (this is especially useful for tracking when any submodules, like common, update) Be a good basis for submitting Pull Requests to be integrated upstream, since it will not contain your local configuration differences or your local GitOps commits Changing subtrees Our patterns use the git subtree feature as a mechanism to promote modularity, so that multiple patterns can use the same common basis. Over time we will move more functionality into common, to isolate the components that are particular to each pattern, and standard usage conventions emerge. This will make the tools in common more powerful and featureful, and make it easier to develop new patterns. Normally, we will maintain the common subtree in the normal course of updates, and pulling changes from upstream will include any changes from common.
You only need to change subtrees if you want to test changes in the common/ area of the pattern repositories, or if you wish to contribute to the common/ repository itself in conjunction with one of the patterns. Using the pattern by itself does not require changing subtrees.
For the common cases (use and consumption of the pattern), users do not need to be aware that the pattern uses a subtree at all.
git clone https://github.com/&lt;your-workspace&gt;/industrial-edge If you want to change and track your own version of common, you should fork and clone our common repository separately:
git clone https://github.com/&lt;your-workspace&gt;/common Now, you can make changes in your fork&rsquo;s main branch, or else make a new branch and make changes there.
If you want to track these changes in your fork of the pattern repository (industrial-edge in this case), you will need to swap out the subtree in industrial-edge for the version of common you forked. We have provided a script to make this a bit easier:
common/scripts/make_common_subtree.sh &lt;subtree_repo&gt; &lt;subtree_branch&gt; &lt;subtree_remote_name&gt; This script will set up a new remote in your local working directory with the repository you specify. It will replace the common directory with a new common from the fork and branch you specify, and commit it. The script will not push the result.
For example:
common/scripts/make_common_subtree.sh https://github.com/mhjacks/common.git wip-main common-subtree This will replace common in the current repository with the wip-main branch from the common in mhjacks&rsquo;s common repository, and call the remote common-subtree.
From that point, changes from mhjacks&rsquo;s wip-main branch on mhjacks&rsquo;s fork of common can be pulled in this way:
git subtree pull --prefix common common-subtree wip-main When run without arguments, the script will run as if it had been given the following arguments:
common/scripts/make_common_subtree.sh https://github.com/hybrid-cloud-patterns/common.git main common-subtree Which are the defaults the repository is normally configured with.
Subtree vs. Submodule It has always been important to us to be have a substrate for patterns that is as easy as possible to share amongst multiple patterns. While it is possible to share changes between multiple unrelated git repositories, it is an almost entirely manual process, prone to error. We feel it is important to be able to provide a &ldquo;pull&rdquo; experience (i.e. one git &ldquo;pull&rdquo; type action) to update the shared components of a pattern. Two strategies exist for repository sharing in this way: submodule and subtree. We started with submodules but have since moved to subtree.
Atlassian has some good documentation on what subtree is here and here. In short, a subtree integrates another repository&rsquo;s history into a parent repository, which allows for most of the benefits of a submodule workflow, without most of the caveats.
Earlier versions of this document described the usage of patterns with submodules instead of subtrees. In the earliest stages of pattern development, we used submodules because the developers of the project were familiar with submodules and had used them previously, but we had not used subtrees. User feedback, as well as some of the unavoidable complexities of submodules, convinced us to try subtrees and we believe we will stick with that strategy. Some of the unavoidable complexities of submodules include:
Having to remember to checkout repositories with --recurse-submdules, or else doing git submodule init &amp;&amp; git submodule sync. Experienced developers asked in several of our support channels early on why common was empty. Hoping that other tools that are interacting with the repository are compatible with the submodule approach. (To be fair, tools like ArgoCD and Tekton Pipelines did this very well; their support of submodules was one of the key reasons we started with submodules) When changing branches on a submoduled repository, if the branch you were changing to was pointed to a different revision of the submoduled repository, the repository would show out of sync. While this behavior is correct, it can be surprising and difficult to navigate. In disconnected environments, submodules require mirroring more repositories. Developing with a fork of the submoduled repository means maintaining two forked repositories and multiple branches in both. Subtrees have some pitfalls as well. In the subtree strategy, it is easier to diverge from the upstream version of the subtree repository, and in fact with a typical git clone, the user may not be aware that a subtree is in use at all. This can be considered a feature, but could become problematic if the user/consumer later wants to update to a newer version of the subtree but local changes might conflict. Additionally, since subtrees are not as well understood generally, there can be some surprising effects. In practice, we have run into the following:
Cherry picking from a subtree commit into the parent puts the change in the parent location, not the subtree Contributing to Patterns using Common Subtrees Once you have forked common and changed your subtree for testing, changes from your fork can then be proposed to [https://github.com/hybrid-cloud-patterns/common.git] and can then be integrated into other patterns. A change to upstream common for a particular upstream pattern would have to be done in two stages:
PR the change into upstream&rsquo;s common PR the updated common into the pattern repository `,url:"https://hybrid-cloud-patterns.io/learn/workflow/",breadcrumb:"/learn/workflow/"},"https://hybrid-cloud-patterns.io/learn/implementation/":{title:"Implementation Requirements",tags:[],content:`Technical Requirements Additional requirements specific to the implementation for all Community, and Validated patterns
Must Patterns MUST include one or more Git repositories, in a publicly accessible location, containing configuration elements that can be consumed by the OpenShift GitOps operator (ArgoCD) without supplying custom ArgoCD images.
Patterns MUST be useful without all content stored in private git repos
Patterns MUST include a list of names and versions of all the products and projects being consumed by the pattern
Patterns MUST be useful without any sample applications that are private or lack public sources.
Patterns must not become useless due to bit rot or opaque incompatibilities in closed source “applications”.
Patterns MUST NOT store sensitive data elements, including but not limited to passwords, in Git
Patterns MUST be possible to deploy on any IPI-based OpenShift cluster (BYO)
We distinguish between the provisioning and configuration requirements of the initial cluster (“Patterns”), and of clusters/machines managed by the initial cluster (see “Managed clusters”)
Patterns MUST use a standardized clustergroup Helm chart, as the initial OpenShift GitOps application that describes all namespaces, subscriptions, and any other GitOps applications which contain the configuration elements that make up the solution.
Managed clusters MUST operate on the premise of “eventual consistency” (automatic retries, and an expectation of idempotence), which is one of the essential benefits of the GitOps model.
Imperative elements MUST be implemented as idempotent code stored in Git
Should Patterns SHOULD include sample application(s) to demonstrate the business problem(s) addressed by the pattern.
Patterns SHOULD try to indicate which parts are foundational as opposed to being for demonstration purposes.
Patterns SHOULD use the VP operator to deploy patterns. However anything that creates the OpenShift GitOps subscription and initial clustergroup application could be acceptable.
Patterns SHOULD embody the “open hybrid cloud model” unless there is a compelling reason to limit the availability of functionality to a specific platform or topology.
Patterns SHOULD use industry standards and Red Hat products for all required tooling
Patterns prefer current best practices at the time of pattern development. Solutions that do not conform to best practices should expect to justify non-conformance and/or expend engineering effort to conform.
Patterns SHOULD NOT make use of upstream/community operators and images except, depending on the market segment, where critical to the overall solution.
Such operators are forbidden to be deployed into an increasing number of customer environments, which limits reuse. Alternatives include productizing the operator, and building it in-cluster from trusted sources as part of the pattern.
Patterns SHOULD be decomposed into modules that perform a specific function, so that they can be reused in other patterns.
For example, Bucket Notification is a capability in the Medical Diagnosis pattern that could be used for other solutions.
Patterns SHOULD use Ansible Automation Platform to drive the declarative provisioning and management of managed hosts (e.g. RHEL). See also “Imperative elements”.
Patterns SHOULD use RHACM to manage policy and compliance on any managed clusters.
Patterns SHOULD use RHACM and a standardized acm chart to deploy and configure OpenShift GitOps to managed clusters.
Managed clusters SHOULD be loosely coupled to their hub, and use OpenShift GitOps to consume applications and configuration directly from Git as opposed to having hard dependencies on a centralized cluster.
Managed clusters SHOULD use the “pull” deployment model for obtaining their configuration.
Imperative elements SHOULD be implemented as Ansible playbooks
Imperative elements SHOULD be driven declaratively – by which we mean that the playbooks should be triggered by Jobs and/or CronJobs stored in Git and delivered by OpenShift GitOps.
Can Patterns CAN include additional configuration and/or demo elements located in one or more additional private git repos. Patterns CAN include automation that deploys a known set of clusters and/or machines in a specific topology Patterns CAN limit functionality/testing claims to specific platforms, topologies, and cluster/node sizes Patterns CAN consume operators from established partners (e.g. Hashicorp Vault, and Seldon) Patterns CAN include managed clusters Patterns CAN include details or automation for provisioning managed clusters, or rely on the admin to pre-provision them out-of-band. Patterns CAN also choose to model multi-cluster solutions as an uncoordinated collection of “initial hub clusters” Imperative elements CAN interact with cluster state and/or external influences `,url:"https://hybrid-cloud-patterns.io/learn/implementation/",breadcrumb:"/learn/implementation/"},"https://hybrid-cloud-patterns.io/learn/community/":{title:"Community Patterns",tags:[],content:`Community Pattern Requirements tl;dr What are they: Best practice implementations conforming to the Validated Patterns implementation practices Purpose: Codify best practices and promote collaboration between different groups inside, and external to, Red Hat Creator: Customers, Partners, GSIs, Services/Consultants, SAs, and other Red Hat teams Requirements General requirements for all Community, and Validated patterns
Base Patterns MUST include a top-level README highlighting the business problem and how the pattern solves it
Patterns MUST include an architecture drawing. The specific tool/format is flexible as long as the meaning is clear.
Patterns MUST undergo an informal architecture review by a community leader to ensure that the solution has the right products, and they are generally being used as intended.
For example: not using a database as a message bus. As community leaders, contributions from within Red Hat may be subject to a higher level of scrutiny While we strive to be inclusive, the community will have quality standards and generally using the framework does not automatically imply a solution is suitable for the community to endorse/publish.
Patterns MUST undergo an informal technical review by a community leader to ensure that it conforms to the technical requirements and meets basic reuse standards
Patterns MUST document their support policy
It is anticipated that most community patterns will be supported by the community on a best-effort basis, but this should be stated explicitly. The validated patterns team commits to maintaining the framework but will also accept help.
Patterns SHOULD include a recorded demo highlighting the business problem and how the pattern solves it
`,url:"https://hybrid-cloud-patterns.io/learn/community/",breadcrumb:"/learn/community/"},"https://hybrid-cloud-patterns.io/learn/validated/":{title:"Validated Patterns",tags:[],content:`Validated Pattern Requirements tl;dr What are they: Technical foundations, backed by CI, that have succeeded in the market and Red Hat expects to be repeatable across customers and segments. Purpose: Reduce risk, accelerate sales cycles, and allow consulting organizations to be more effective. Creator: The Validated Patterns team in conjunction with: Partners, GSIs, Services/Consultants, SAs, and other Red Hat teams Onboarding Existing Implementations The Validated Patterns team has a preference for empowering other, and not taking credit for their work.
Where there is an existing application/demo, there is also a strong preference for the originating team to own any changes that are needed for the implementation to become a validated pattern. Alternatively, if the Validated Patterns team drives the conversion, then in order to prevent confusion and duplicated efforts, we are likely to ask for a commitment to phase out use of the previous implementation for future engagements such as demos, presentations, and workshops.
The goal is to avoid bringing a parallel implementation into existence which divides Red Hat resources, and creates confusion internally and with customers as the implementations drift apart.
In both scenarios the originating team can choose where to host the primary repository, will be given admin permissions to any fork in https://github.com/hybrid-cloud-patterns, and will receive on-going assistance from the Validated Patterns team.
Nominating a Community Pattern to become Validated If there is a community pattern that you believe would be a good candidate for becoming validated, please email hybrid-cloud-patterns@googlegroups.com at least 4 weeks prior to the end of a given quarter in order for the necessary work to be considered as part of the following quarter’s planning process.
Please be aware that each Validated Pattern represents an ongoing maintenance, support, and CI effort. Finite team capacity means we must critically balance this cost against the potential customer opportunity. A “no” or “not yet” result is not intended as an insult against the pattern or its author.
Requirements Validated Patterns have deliverable and requirements in addition to those specified for Community-level patterns
Must Validated Patterns MUST contain more than two RH products. Alternative: Engage with the BU
Validated Patterns, or the solution on which they are based, MUST have been deployed and approved for use in at least one customer environment.
Alternative: Community Pattern
Validated Patterns MUST be meaningful without specialized hardware, including flavors of architectures not explicitly supported. Alternative: Engage with DCI
Qualification is a Validated Patterns engineering decision with input from the pattern owner.
Validated Patterns MUST be broadly applicable. Alternative: Engage with Phased Gate and/or TAMs
Qualification is a Validated Patterns PM decision with input from the pattern owner.
Validated Patterns MUST only make use of Red Hat products that are already fully supported by their product team(s).
Validated Patterns MUST NOT rely on functionality in tech-preview, or hidden behind feature gates.
Validated Patterns MUST conform to the same Community-level implementation requirements
Validated Patterns MUST have their architectures reviewed by the PM, TPM, or TMM of each Red Hat product they consume to ensure consistency with the product teams’ intentions and roadmaps
Validated Patterns MUST have their implementation reviewed by the patterns team to ensure that it is sufficiently flexible to function across a variety of platforms, customer environments, and any relevant verticals.
Validated Patterns MUST include a standardized architecture drawing, created with (or at least conforming to) the PAC tooling
Validated Patterns MUST include a presentation deck oriented around the business problem being solved and intended for use by the field to sell and promote the solution
Validated Patterns MUST include a recorded demo highlighting the business problem and how the pattern solves it
Validated Patterns MUST include a test plan covering all features or attributes being highlighted by the demo that also spans multiple products. Negative flow tests (such as resiliency or data retention in the presence of network outages) are limited to scenarios covered by the demonstration script.
Validated Patterns MUST include automated CI testing that runs on every change to the pattern, or a schedule no less frequently than once per week
Validated Patterns MUST create a new point release of the validation-level deliverables when minor versions (e.g. “12” in OpenShift 4.12) of consumed products change
Validated Patterns MUST document their support policy
The individual products used in a Validated Pattern are backed by the full Red Hat support experience conditional on the customer’s subscription to those products, and the individual products’ support policy. Additional components in a Validated Pattern that are not supported by Red Hat (e.g. Hashicorp Vault, and Seldon Core) will require a customer to obtain support from that vendor directly. The validated patterns team is very motivated to address any problems in the VP Operator, as well as problems in the common helm charts, but cannot not offer any SLAs at this time. See also our standard disclaimer
Validated Patterns DO NOT imply an obligation of support for partner or community operators by Red Hat.
Should Validated Patterns SHOULD focus on functionality not performance.
Validated Patterns SHOULD trigger CI runs for new versions of consumed products
Validated Patterns SHOULD provide an RHPDS lab environment
A bare bones environment into which the solution can be deployed, and a list of instructions for doing so (e.g. installing and configuring OpenShift GitOps)
Validated Patterns SHOULD provide pre-built demo environment using RHPDS
Having an automated demo within the RHPDS system, that will be built based on the current stable version that is run against the CI testing system
Validated Patterns SHOULD track deployments of each validation-level deliverable
For lifecycle decisions like discontinuing support of a version For notification if problems are found in our CI
Can Teams creating Validated Patterns CAN provide their own SLA
A document for QE that defines, at a technical level, how to validate if the pattern has been successfully deployed and is functionally operational. Example: Validating an Industrial Edge Deployment
`,url:"https://hybrid-cloud-patterns.io/learn/validated/",breadcrumb:"/learn/validated/"},"https://hybrid-cloud-patterns.io/learn/secrets/":{title:"Secrets",tags:[],content:`Infrastructure Background Enterprise applications require security, especially in multi-cluster and multi-site environments. Applications require trust and use certificates and other secrets in order to establish and maintain trust. In this section we will look at various ways of managing secrets.
When you start developing distributed enterprise applications there is a strong temptation to ignore security during development and add it at the end. This is proven to be a very bad practice that accumulates technical debt that sometimes never gets resolved.
While the DevOps model of development strongly encourages shifting security to the left many developers didn&rsquo;t really take notice and so the more explicit term DevSecOps was created. Essentially, &ldquo;pay attention and consider and implement security as early as possible in the lifecycle&rdquo;. (i.e. shift left on the time line).
Secret Management One area that has been impacted by a more automated approach to security is in the secret management. DevOps (and DevSecOps) environments require the use of many different services:
Code repositories GitOps tools Image repositories Build pipelines All of these services require credentials. (Or should do!) And keeping those credentials secret is very important. E.g. pushing your credentials to your personal GitHub/GitLab repository is not a secure solution.
While using a file based secret management can work if done correctly, most organizations opt for a more enterprise solution using a secret management product or project. The Cloud Native Computing Foundation (CNCF) has many such projects. The Hybrid Cloud Patterns project has started with Hashicorp Vault secret management product but we look forward to other project contributions.
What&rsquo;s next? Getting started with Vault
`,url:"https://hybrid-cloud-patterns.io/learn/secrets/",breadcrumb:"/learn/secrets/"},"https://hybrid-cloud-patterns.io/learn/vault/":{title:"HashiCorp Vault",tags:[],content:`Deploying HashiCorp Vault in a validated pattern Prerequisites You have deployed/installed a validated pattern using the instructions provided for that pattern. This should include setting having logged into the cluster using oc login or setting you KUBECONFIG environment variable and running a make install.
Setting up HashiCorp Vault Any validated pattern that uses HashiCorp Vault already has deployed Vault as part of the make install. To verify that Vault is installed you can first see that the vault project exists and then select the Workloads/Pods:
In order to setup HashiCorp Vault there are two different ways, both of which happen automatically as part of the make install command:
Inside the cluster directly when the helm value clusterGroup.insecureUnsealVaultInsideCluster is set to true. With this method a cronjob will run every five minutes inside the imperative namespace and unseal, initialize and configure the vault. The vault&rsquo;s unseal keys and root token will be stored inside a secret called vaultkeys in the imperative namespace. It is considered best practice to copy the content of that secret offline, store it securely and then delete it. On the user&rsquo;s computer when the helm value clusterGroup.insecureUnsealVaultInsideCluster is set to false. This will store the json containing containing both vault root token and unseal keys inside a file called common/pattern-vault.init. It is recommended to encrypt this file or store it securely. An example output is the following:
{ &#34;recovery_keys_b64&#34;: [], &#34;recovery_keys_hex&#34;: [], &#34;recovery_keys_shares&#34;: 0, &#34;recovery_keys_threshold&#34;: 0, &#34;root_token&#34;: &#34;hvs.VNFq7yPuZljq2VDJTkgAMs2Z&#34;, &#34;unseal_keys_b64&#34;: [ &#34;+JJjKgZyEB1rbKlXs1aTuC+PBivukIlnpoe7bH4qc7TL&#34;, &#34;X2ib6LNZw+kOQH1WYR9t3RE2SgB5WbEf2FfD40OybNXf&#34;, &#34;A4DIhv9atLIQsqqyDAYkmfEJPYhFVuKGSGYwV7WCtGcL&#34;, &#34;ZWkQ7+qtgmClKdlNKWcdpvyxArm07P9eArHZB4/CMZWn&#34;, &#34;HXakF073+Kk7oOpAFbGlKIWYApzUhC/F1LDfowF/M1LK&#34; ], &#34;unseal_keys_hex&#34;: [ &#34;f892632a0672101d6b6ca957b35693b82f8f062bee908967a687bb6c7e2a73b4cb&#34;, &#34;5f689be8b359c3e90e407d56611f6ddd11364a007959b11fd857c3e343b26cd5df&#34;, &#34;0380c886ff5ab4b210b2aab20c062499f1093d884556e28648663057b582b4670b&#34;, &#34;656910efeaad8260a529d94d29671da6fcb102b9b4ecff5e02b1d9078fc23195a7&#34;, &#34;1d76a4174ef7f8a93ba0ea4015b1a5288598029cd4842fc5d4b0dfa3017f3352ca&#34; ], &#34;unseal_shares&#34;: 5, &#34;unseal_threshold&#34;: 3 } The vault&rsquo;s root token is needed to log into the vault&rsquo;s UI and the unseal keys are needed whenever the vault pods are restarted. In the OpenShift console click on the nine box at the top and click on the vault line: []
Copy the root_token field which in the example above has the value hvs.VNFq7yPuZljq2VDJTkgAMs2Z and paste it in the sign-in page:
After signing in you will see the secrets that have been created.
Unseal If you don&rsquo;t see the sign in page but instead see an unseal page, something may have happened the cluster and you need to unseal it again. Instead of using make vault-init you should run make vault-unseal. You can also unseal it manually by running vault operator unseal inside the vault-0 pod in the vault namespace.
What&rsquo;s next? Check with the validated pattern instructions to see if there are further steps you need to perform. Sometimes this might be deploying a pattern on an edge cluster and checking to see if the correct Vault handshaking and updating occurs.
`,url:"https://hybrid-cloud-patterns.io/learn/vault/",breadcrumb:"/learn/vault/"},"https://hybrid-cloud-patterns.io/learn/faq/":{title:"FAQ",tags:[],content:`FAQ What is a Hybrid Cloud Pattern? Hybrid Cloud Patterns are collections of applications (in the ArgoCD sense) that demonstrate aspects of hub/edge computing that seem interesting and useful. Hybrid Cloud Patterns will generally have a hub or centralized component, and an edge component. These will interact in different ways.
Many things have changed in the IT landscape in the last few years - containers and kubernetes have taken the industry by storm, but they introduce many technologies and concepts. It is not always clear how these technologies and concepts play together - and Hybrid Cloud Patterns is our effort to show these technologies working together on non-trivial applications in ways that make sense for real customers and partners to use.
The first Hybrid Cloud Pattern is based on MANUela, an application developed by Red Hat field associates. This application highlights some interesting aspects of the industrial edge in a cloud-native world - the hub component features pipelines to build the application, a &ldquo;twin&rdquo; for testing purposes, a central data lake, an s3 component to gather data from the edge installations (which are factories in this case). The edge component has machine sensors, which are responsible for only gathering data from instrumented line devices and shares them via MQTT messaging. The edge also features Seldon, an AI/ML framework for making predictions, a custom Node.js application to show data in real time, and messaging components supporting both MQTT and Kafka protocols. The local applications use MQTT to retrieve data for display, and the Kafka components move the data to the central hub for storage and analysis.
We are actively developing new Hybrid Cloud Patterns. Watch this space for updates!
How are they different from XYZ? Many technology demos can be very minimal - such demos have an important place in the ecosystem to demonstrate the intent of an individual technology. Hybrid Cloud Patterns are meant to demonstrate groups of technologies working together in a cloud native way. And yet, we hope to make these patterns general enough to allow for swapping application components out &ndash; for example, if you want to swap out ActiveMQ for RabbitMQ to support MQTT - or use a different messaging technology altogether, that should be possible. The other components will require reconfiguration.
What technologies are used? Key technologies in the stack for Industrial Edge include:
Red Hat OpenShift Container Platform Red Hat Advanced Cluster Management Red Hat OpenShift GitOps (based on ArgoCD) Red Hat OpenShift Pipelines (based on tekton) Red Hat Integration - AMQ Broker (ActiveMQ Artemis MQTT) Red Hat Integration - AMQ Streams (Kafka) Red Hat Integration - Camel K Seldon Operator In the future, we expect to further use Red Hat OpenShift, and expand the integrations with other elements of the ecosystem. How can the concept of GitOps integrate with a fleet of devices that are not running Kubernetes? What about integrations with baremetal or VM servers? Sounds like a job for Ansible! We expect to tackle some of these problems in future patterns.
How are they structured? Hybrid Cloud Patterns come in parts - we have a common repository with logic that will apply to multiple patterns. Layered on top of that is our first pattern - industrial edge. This layout allows for individual applications within a pattern to be swapped out by pointing to different repositories or branches for those individual components by customizing the values files in the root of the repository to point to different branches or forks or even different repositories entirely. (At present, the repositories all have to be on github.com and accessible with the same token.)
The common repository is primarily concerned with how to deploy the GitOps operator, and to create the namespaces that will be necessary to manage the pattern applications.
The pattern repository has the application-specific layout, and determines which components are installed in which places - hub or edge. The pattern repository also defines the hub and edge locations. Both the hub and edge are expected to have multiple components each - the hub will have pipelines and the CI/CD framework, as well as any centralization components or data analysis components. Edge components are designed to be smaller as we do not need to deploy Pipelines or the test and staging areas to the Edge.
Each application is described as a series of resources that are rendered into GitOps (ArgoCD) via Helm and Kustomize. The values for these charts are set by values files that need to be &ldquo;personalized&rdquo; (with your local cluster values) as the first step of installation. Subsequent pushes to the gitops repository will be reflected in the clusters running the applications.
Who is behind this? Today, a team of Red Hat engineers including Andrew Beekhof (@beekhof), Lester Claudio (@claudiol), Martin Jackson (@mhjacks), William Henry (@ipbabble), Michele Baldessari (@mbaldessari), Jonny Rickard (@day0hero) and others.
Excited or intrigued by what you see here? We&rsquo;d love to hear your thoughts and ideas! Try the patterns contained here and see below for links to our repositories and issue trackers.
How can I get involved? Try out what we&rsquo;ve done and submit issues to our issue trackers.
We will review pull requests to our pattern repositories.
`,url:"https://hybrid-cloud-patterns.io/learn/faq/",breadcrumb:"/learn/faq/"},"https://hybrid-cloud-patterns.io/":{title:"Hybrid Cloud Patterns",tags:[],content:`
Reference architectures with added value Hybrid Cloud Patterns are an evolution of how you deploy applications in a hybrid cloud. With a pattern, you can automatically deploy a full application stack through a GitOps-based framework. With this framework, you can create business-centric solutions while maintaining a level of Continuous Integration (CI) over your application.
`,url:"https://hybrid-cloud-patterns.io/",breadcrumb:"/"},"https://hybrid-cloud-patterns.io/learn/":{title:"Learn about Hybrid Cloud Patterns",tags:[],content:`Find out more information about Hybrid Cloud Patterns and how they work.
`,url:"https://hybrid-cloud-patterns.io/learn/",breadcrumb:"/learn/"}}</script><script src=/js/lunr.js></script>
<script src=/js/search.js></script></footer></main></div></body></html>